{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sine Curve Regression\n",
    "\n",
    "This is a simple example of using tensorflow to create a regressions of a sine curve from 0 to 4$\\pi$. It has an input layer, 3 hidden layers, and an output layer.\n",
    "\n",
    "Things to improve on this would be inserting data in batches and randomizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "empty = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the different varible's we'll need are declared. \"epochs\" is how many times to run through each layer. \"layer_nodes\" are the input and output size of each layer. \"act_funs\" is the function to be used for each layer, and \"learning_rate\" is how aggresive the NN is at correcting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "layer_nodes = [1, 64, 64, 64, 64, 1]\n",
    "act_funcs = [tf.tanh, tf.nn.leaky_relu, tf.nn.leaky_relu, tf.tanh, empty]\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our function to make our layer. It takes W, our weights, and b, the bias, and inserts them into the activation functions, and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(act_func, input_val, input_num, output_num):\n",
    "    W = tf.Variable(tf.random_normal([input_num, output_num], stddev=0.03), dtype=tf.float32)\n",
    "    b = tf.Variable(tf.random_normal([output_num], stddev=0.03), dtype=tf.float32)\n",
    "    layer = act_func(tf.matmul(input_val,W) + b)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a placeholder for x, it'll be our slot to feed data into our model. Right after the data itself is created, and then the layers using our layer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#the actual data\n",
    "x_s = np.linspace(0, 4*np.pi, 500)\n",
    "x_2 = x_s\n",
    "x_train = x_2.reshape(-1,1)\n",
    "y_train_tf = tf.sin(x)\n",
    "\n",
    "#creating the layers\n",
    "layer1 = layer(act_funcs[0], x, layer_nodes[0], layer_nodes[1])\n",
    "layer2 = layer(act_funcs[1], layer1, layer_nodes[1], layer_nodes[2])\n",
    "layer3 = layer(act_funcs[2], layer2, layer_nodes[2], layer_nodes[3])\n",
    "layer4 = layer(act_funcs[3], layer3, layer_nodes[3], layer_nodes[4])\n",
    "layer5 = layer(act_funcs[4], layer4, layer_nodes[4], layer_nodes[5])\n",
    "output = layer5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our loss function is made. It's defined as as the mean squared error of the actuall y data, and what our function is outputting. A Gradient Descent is then used to minimize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_train_tf, output)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. We begin a tensorflow session, and then iterate through all our epochs, continously feeding our data in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs+1):\n",
    "        predict, _ = sess.run([output, train], feed_dict={x: x_train})\n",
    "        y_train, curr_loss = sess.run([y_train_tf, loss], feed_dict={x: x_train})\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch: %s \\t Loss: %s\" % (i, curr_loss))\n",
    "\n",
    "ax.plot(x_s, y_train[:],'b-')\n",
    "ax.plot(x_s, predict, 'r--')\n",
    "ax.set(xlabel='X Value', ylabel='Y / Predicted Value', title=[str(i),\" Loss: \", curr_loss])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
