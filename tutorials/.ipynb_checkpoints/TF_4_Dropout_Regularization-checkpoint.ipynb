{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout & Regularization\n",
    "\n",
    "In this tutorial, we'll cover new concepts such as dropout and regularization. Note just like last time this'll be based on the previous example, so only new concepts will be covered. Also note the function being modeled has changed to $X^3+Y^2 = Z$, to increase the difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import Utils as ut\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our functions that'll be used throughout just like last time, but the main changes in this tutorial will all happen in the make_layer function. First, notice the regularization function. Along with our normal losses, we'll be tracking the magnitude of each weight using the L2 norm (this is also commonly done with the L1 norm). Later we'll incoporate this into our overall loss. This prevents any weight from getting too large and thus making our layer unbalanced and preventing overfitting.\n",
    "\n",
    "We also have dropout a few lines later. Dropout basically allows us to skip nodes randomly in each layer, removing overdependence on any one weight and preventing overfitting once again. The \"keep_prob\" parameter ranges from 0 to 1, and is the likelihood that that node will be skipped. For example, if keep_prob is .90, then it'll have a 90% chance of skipping that node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(act_func, input_val, input_num, output_num, keep_prob):\n",
    "    W = tf.Variable(tf.random_normal([input_num, output_num], stddev=0.03), dtype=tf.float32)\n",
    "    b = tf.Variable(tf.random_normal([output_num], stddev=0.03), dtype=tf.float32)\n",
    "    layer = act_func(tf.matmul(input_val,W) + b)\n",
    "    regularization = tf.nn.l2_loss(W)\n",
    "    if input_num == output_num:\n",
    "        layer = tf.nn.dropout(layer, keep_prob)\n",
    "    return layer, regularization\n",
    "\n",
    "linear = lambda x: x\n",
    "\n",
    "def make_r(y, pred):\n",
    "    if y.shape[1] == 2:\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y[:,0:1], tf.reduce_mean(y[:,0:1])))) + tf.reduce_sum(tf.square(tf.subtract(y[:,1:2], tf.reduce_mean(y[:,1:2]))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y[:,0:1], pred[:,0:1]))) + tf.reduce_sum(tf.square(tf.subtract(y[:,1:2], pred[:,1:2])))\n",
    "        R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "        return R_squared\n",
    "    elif y.shape[1] == 1:\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, pred)))\n",
    "        R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "        return R_squared\n",
    "    else:\n",
    "        raise TypeError(\"Weird Shape for R Squared\")\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.001\n",
    "num_points = 10\n",
    "num_nodes = 256\n",
    "num_batch = 30\n",
    "num_layers = 5\n",
    "drop_prob = 0.9\n",
    "beta = 0.0001\n",
    "\n",
    "#used for inputting into layers\n",
    "input_tf = tf.placeholder(tf.float32, [None,4])\n",
    "output_tf = tf.placeholder(tf.float32, [None,2])\n",
    "np.random.seed()\n",
    "#actual data points\n",
    "x_r = np.linspace(0, 20, num_points)\n",
    "x_i = np.linspace(-10, 0, num_points)\n",
    "y_r = np.linspace(-30, 10, num_points)\n",
    "y_i = np.linspace(20, 30, num_points)\n",
    "x_f = x_r\n",
    "y_f = y_r\n",
    "x_r, y_r, x_i, y_i = np.meshgrid(x_r, y_r, x_i, y_i)\n",
    "\n",
    "#prepping data to be input\n",
    "x_r = x_r.reshape(-1,1)\n",
    "x_i = x_i.reshape(-1,1)\n",
    "y_r = y_r.reshape(-1,1)\n",
    "y_i = y_i.reshape(-1,1)\n",
    "z = (x_r+x_i*1j)**3 + (y_r+y_i*1j)**2\n",
    "data = np.hstack((x_r, x_i, y_r, y_i, z.real, z.imag))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "#split into training and validation sets\n",
    "train_size = int(len(x_r) * 0.7)\n",
    "data_tr = data[0:train_size]\n",
    "data_val = data[train_size:]\n",
    "\n",
    "#normalizing data\n",
    "norm  = MinMaxScaler(copy=True, feature_range=(-1,1))\n",
    "norm.fit(data_tr)\n",
    "data_tr = norm.transform(data_tr)\n",
    "data_val = norm.transform(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our layers our created a little differently. We define keep_prob as a placeholder so that we can turn on and off whether nodes are skipped when actually feeding the data in. This allows us to turn off dropout when getting our predictions out.\n",
    "\n",
    "We also have the regularizer parameter that sums up the L2 norm of the weights of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "input, regularizer = make_layer(tf.nn.leaky_relu, input_tf, 4, num_nodes, keep_prob)\n",
    "hidden = input\n",
    "for i in range(num_layers):\n",
    "    hidden, temp = make_layer(tf.nn.leaky_relu, hidden, num_nodes, num_nodes, keep_prob)\n",
    "    regularizer += temp\n",
    "output, temp = make_layer(linear, hidden, num_nodes, 2, keep_prob)\n",
    "regularizer += temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reduce the mean of loss_real, loss_imag and our regularizer value times some beta defined above to scale it to the value we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_real = tf.losses.mean_squared_error(output_tf[:,0:1], output[:,0:1])\n",
    "loss_imag = tf.losses.mean_squared_error(output_tf[:,1:2], output[:,1:2])\n",
    "loss = tf.reduce_mean(loss_real + loss_imag + regularizer*beta)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#to calculate R^2\n",
    "r = make_r(output_tf, output)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our session runs the same as before, only this time with keep_prob as part of the feed_dict. Notice it'll have value drop_prob as defined above when training, but when calculating losses, $R^2$, and our prediction it'll have a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(data_tr) // num_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    plot_epoch = []\n",
    "    plot_loss_tr = []\n",
    "    plot_loss_val = []\n",
    "    plot_r_tr = []\n",
    "    plot_r_val = []\n",
    "    count = 0\n",
    "    for i in range(epochs+1):\n",
    "        for j in range(0,len(data_tr), batch_size):\n",
    "            sess.run(train, feed_dict={input_tf: data_tr[j:j+batch_size,0:4], output_tf: data_tr[j:j+batch_size,4:6], keep_prob: drop_prob})\n",
    "\n",
    "        #get losses\n",
    "        feed_tr = {input_tf: data_tr[:,0:4], output_tf: data_tr[:,4:6], keep_prob: 1}\n",
    "        feed_val = {input_tf: data_val[:,0:4], output_tf: data_val[:,4:6], keep_prob: 1}\n",
    "        plot_epoch += [i]\n",
    "        plot_loss_tr.append(loss_real.eval(feed_dict=feed_tr) + loss_imag.eval(feed_dict=feed_tr))\n",
    "        plot_loss_val.append(loss_real.eval(feed_dict=feed_val) + loss_imag.eval(feed_dict=feed_val))\n",
    "        #get r\n",
    "        plot_r_tr.append(r.eval(feed_dict=feed_tr))\n",
    "        plot_r_val.append(r.eval(feed_dict=feed_val))\n",
    "        #jump ship if it's good enough\n",
    "        if len(plot_loss_tr) > 1 and abs(plot_loss_tr[-1]-plot_loss_tr[-2]) < 0.00001:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        if count >= 2:\n",
    "            break\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch: %s \\t TLoss: %s \\t VLoss: %s\" % (i, plot_loss_tr[-1], plot_loss_val[-1]))\n",
    "\n",
    "    #get predictions\n",
    "    pred_tr = output.eval(feed_dict=feed_tr)\n",
    "    pred_val = output.eval(feed_dict=feed_val)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(221)\n",
    "ax1.plot(plot_epoch, plot_loss_tr, 'b-', lw=0.5, label=\"Training Set\")\n",
    "ax1.plot(plot_epoch, plot_loss_val,'r-', lw=0.5, label=\"Validation Set\")\n",
    "ax1.axis([0,len(plot_epoch),0,.1])\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MSE\")\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "ax2.plot(plot_epoch, plot_r_tr,'b-', lw=0.5)\n",
    "ax2.plot(plot_epoch, plot_r_val,'r-', lw=0.5)\n",
    "ax2.axis([0,len(plot_epoch),0.9,1.0001])\n",
    "ax2.set_title(\"R Squared\")\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3, projection='3d')\n",
    "ax3.set_title(\"Real Part\")\n",
    "ax4 = plt.subplot(2, 2, 4, projection='3d')\n",
    "ax4.set_title(\"Imaginary Part\")\n",
    "\n",
    "data_tr[:,4:6] = np.array(pred_tr)\n",
    "data_tr = norm.inverse_transform(data_tr)\n",
    "ax3.scatter3D(data_tr[:,0:1], data_tr[:,2:3], data_tr[:,4:5], marker=\".\", c='b')\n",
    "ax4.scatter3D(data_tr[:,1:2], data_tr[:,3:4], data_tr[:,5:6], marker=\".\", c='b')\n",
    "\n",
    "data_val[:,4:6] = np.array(pred_val)\n",
    "data_val = norm.inverse_transform(data_val)\n",
    "ax3.scatter3D(data_val[:,0:1], data_val[:,2:3], data_val[:,4:5], marker=\".\", c='r')\n",
    "ax4.scatter3D(data_val[:,1:2], data_val[:,3:4], data_val[:,5:6], marker=\".\", c='r')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
