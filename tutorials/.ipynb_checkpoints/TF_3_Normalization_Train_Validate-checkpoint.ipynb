{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Training/Validation Sets\n",
    "\n",
    "This will be a quick intro on how to normalize data for use in the NN, as well as how to properly use training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import Utils as ut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.0005\n",
    "num_points = 15\n",
    "num_nodes = 64\n",
    "num_batch = 30\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all our functions we'll need. Notice our make_layer function is the same as before as is 'linear'. The $R^2$ function is new however. $R^2$ is a way of mapping correlation between our output data and what the NN outputs to check for a correlation. The closer it is to 1, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = lambda x: x\n",
    "\n",
    "def make_layer(act_func, input_val, input_num, output_num):\n",
    "    W = tf.Variable(tf.random_normal([input_num, output_num], stddev=0.03), dtype=tf.float32)\n",
    "    b = tf.Variable(tf.random_normal([output_num], stddev=0.03), dtype=tf.float32)\n",
    "    layer = act_func(tf.matmul(input_val,W) + b)\n",
    "    return layer\n",
    "\n",
    "def make_r(y, pred):\n",
    "    if y.shape[1] == 2:\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y[:,0:1], tf.reduce_mean(y[:,0:1])))) + tf.reduce_sum(tf.square(tf.subtract(y[:,1:2], tf.reduce_mean(y[:,1:2]))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y[:,0:1], pred[:,0:1]))) + tf.reduce_sum(tf.square(tf.subtract(y[:,1:2], pred[:,1:2])))\n",
    "        R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "        return R_squared\n",
    "    elif y.shape[1] == 1:\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, pred)))\n",
    "        R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "        return R_squared\n",
    "    else:\n",
    "        raise TypeError(\"Weird Shape for R Squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are placeholders, they are like empty slots where we can feed things into the NN machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tf = tf.placeholder(tf.float32, [None,4])\n",
    "output_tf = tf.placeholder(tf.float32, [None,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making data to put input, using meshgrid to make all possible multiples of inputs. There is both real and imaginary parts for both x and y. \n",
    "\n",
    "Right after the z values are calculated (could be given to us as well) and everything is made into a column vector and stuck into a 6 column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r = np.linspace(0, 20, num_points)\n",
    "x_i = np.linspace(-10, 0, num_points)\n",
    "y_r = np.linspace(-30, 10, num_points)\n",
    "y_i = np.linspace(20, 30, num_points)\n",
    "x_r, y_r, x_i, y_i = np.meshgrid(x_r, y_r, x_i, y_i)\n",
    "\n",
    "x_r = x_r.reshape(-1,1)\n",
    "x_i = x_i.reshape(-1,1)\n",
    "y_r = y_r.reshape(-1,1)\n",
    "y_i = y_i.reshape(-1,1)\n",
    "z = (x_r+x_i*1j) + (y_r+y_i*1j)\n",
    "data = np.hstack((x_r, x_i, y_r, y_i, z.real, z.imag))\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right here we split our data into a training set and a validation set. The training set is for actually training the NN, and the validation set is to make sure it isn't introducing bias into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(x_r) * 0.7)\n",
    "data_tr = data[0:train_size]\n",
    "data_val = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the data is normalized, using a normalizer from sklearn. This one in particularly scales all the data to be within -1 and 1 (0 and 1 by default). Other normalizers will also modify std's along with scaling.\n",
    "\n",
    "Notice the normalizer is formed from the training set only, and then applied to both the training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = MinMaxScaler(copy=True, feature_range=(-1,1))\n",
    "norm.fit(data_tr)\n",
    "data_tr = norm.transform(data_tr)\n",
    "data_val = norm.transform(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where are layers are made, using the make_layer function found above. Note that there's 4 inputs, and make make \"num_layers\" layers and \"num_nodes\" nodes per layer, and then an output of 2, one real and one imaginary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = make_layer(tf.nn.leaky_relu, input_tf, 4, num_nodes)\n",
    "hidden = [input]\n",
    "for i in range(num_layers):\n",
    "    hidden += [make_layer(tf.nn.leaky_relu, hidden[-1], num_nodes, num_nodes)]\n",
    "output = make_layer(linear, hidden[-1], num_nodes, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where our loss functions are made using MSE, one for the real part and one for the imaginary. Notice that we will be comparing the output that we put in (using the output_tf placeholder) and the output the NN will spit out.\n",
    "\n",
    "$R^2$ is also calculated here using the make_r function also found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_real = tf.losses.mean_squared_error(output_tf[:,0:1], output[:,0:1])\n",
    "loss_imag = tf.losses.mean_squared_error(output_tf[:,1:2], output[:,1:2])\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss_imag+loss_real)\n",
    "\n",
    "r = make_r(output_tf, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. We'll begin a session to start training our neural network. All the empty lists will be used to store the loss and $R^2$ value at each epoch.\n",
    "\n",
    "The first for loop runs it through each epoch exactly once, while the second runs it through all the differents batches. Note inside the second for loop we're doing \"sess.run\" on the train object, and passing in only values from the training set.\n",
    "\n",
    "Later on we pass the loss functions and $R^2$ function to store those for later. There's also some if statement to exit session early if the loss function's decrease stagnates. And at the end after all the epoch are ran, the actual data is stored to use in plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = len(data_tr) // num_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    plot_epoch = []\n",
    "    plot_loss_tr = []\n",
    "    plot_loss_val = []\n",
    "    plot_r_tr = []\n",
    "    plot_r_val = []\n",
    "    count = 0\n",
    "    for i in range(epochs+1):\n",
    "        for j in range(0,len(data_tr), batch_size):\n",
    "            sess.run(train, feed_dict={input_tf: data_tr[j:j+batch_size,0:4], output_tf: data_tr[j:j+batch_size,4:6]})\n",
    "\n",
    "        #get losses\n",
    "        plot_epoch += [i]\n",
    "        temp1, temp2 = sess.run([loss_real,loss_imag], feed_dict={input_tf: data_tr[:,0:4], output_tf: data_tr[:,4:6]})\n",
    "        plot_loss_tr += [temp1 + temp2]\n",
    "        temp1, temp2 = sess.run([loss_real,loss_imag], feed_dict={input_tf: data_val[:,0:4], output_tf: data_val[:,4:6]})\n",
    "        plot_loss_val += [temp1 + temp2]\n",
    "        #get r\n",
    "        plot_r_tr += sess.run([r], feed_dict={input_tf: data_tr[:,0:4], output_tf: data_tr[:,4:6]})\n",
    "        plot_r_val += sess.run([r], feed_dict={input_tf: data_val[:,0:4], output_tf: data_val[:,4:6]})\n",
    "        #jump ship if it's good enough\n",
    "        if len(plot_loss_tr) > 1 and abs(plot_loss_tr[-1]-plot_loss_tr[-2]) < 0.00001:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        if count >= 2:\n",
    "            break\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch: %s \\t TLoss: %s \\t VLoss: %s\" % (i, plot_loss_tr[-1], plot_loss_val[-1]))\n",
    "\n",
    "    #get predictions\n",
    "    pred_tr = sess.run([output], feed_dict={input_tf: data_tr[:,0:4], output_tf: data_tr[:,4:6]})\n",
    "    pred_val = sess.run([output], feed_dict={input_tf: data_val[:,0:4], output_tf: data_val[:,4:6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where everything is plotted, including MSE, $R^2$, and the comparison between training and validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(221)\n",
    "ax1.plot(plot_epoch, plot_loss_tr, 'b-', lw=0.5, label=\"Training Set\")\n",
    "ax1.plot(plot_epoch, plot_loss_val,'r-', lw=0.5, label=\"Validation Set\")\n",
    "ax1.axis([0,len(plot_epoch),0,.01])\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MSE\")\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "ax2.plot(plot_epoch, plot_r_tr,'b-', lw=0.5)\n",
    "ax2.plot(plot_epoch, plot_r_val,'r-', lw=0.5)\n",
    "ax2.axis([0,len(plot_epoch),0.99,1.0001])\n",
    "ax2.set_title(\"R Squared\")\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3, projection='3d')\n",
    "ax3.set_title(\"Real Part\")\n",
    "ax4 = plt.subplot(2, 2, 4, projection='3d')\n",
    "ax4.set_title(\"Imaginary Part\")\n",
    "\n",
    "data_tr[:,4:6] = np.array(pred_tr)\n",
    "data_tr = norm.inverse_transform(data_tr)\n",
    "ax3.scatter3D(data_tr[:,0:1], data_tr[:,2:3], data_tr[:,4:5], marker=\".\", c='b')\n",
    "ax4.scatter3D(data_tr[:,1:2], data_tr[:,3:4], data_tr[:,5:6], marker=\".\", c='b')\n",
    "\n",
    "data_val[:,4:6] = np.array(pred_val)\n",
    "data_val = norm.inverse_transform(data_val)\n",
    "ax3.scatter3D(data_val[:,0:1], data_val[:,2:3], data_val[:,4:5], marker=\".\", c='r')\n",
    "ax4.scatter3D(data_val[:,1:2], data_val[:,3:4], data_val[:,5:6], marker=\".\", c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
